{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e96b23b",
   "metadata": {},
   "source": [
    "# Understanding the Character-Level LSTM Model\n",
    "\n",
    "This notebook utilizes a character-level language model based on Long Short-Term Memory (LSTM) networks, implemented in `src/myprogram.py`. Here's a brief overview:\n",
    "\n",
    "**Model Architecture:**\n",
    "\n",
    "The core of the model (`CharLSTM` class) consists of:\n",
    "1.  **Embedding Layer:** Converts input characters into dense vector representations. Each unique character in the training data gets its own vector.\n",
    "2.  **LSTM Layer(s):** These recurrent neural network layers process the sequence of character embeddings. LSTMs are designed to capture dependencies and context over varying lengths in sequential data, making them suitable for text.\n",
    "3.  **Fully Connected (Linear) Layer:** Takes the LSTM's output and projects it onto the vocabulary space, producing a score (logit) for each possible next character.\n",
    "4.  **Dropout:** Incorporated to prevent overfitting during training.\n",
    "\n",
    "**Data Processing (`CharDataset` class):**\n",
    "*   Text data is transformed into sequences of a fixed length (`seq_length`).\n",
    "*   A vocabulary is built, mapping each unique character to an integer index.\n",
    "*   For each input sequence, the target is the character immediately following it.\n",
    "\n",
    "**Functionality (`MyModel` class):**\n",
    "*   **Training (`run_train`):** The model learns to predict the next character by minimizing the cross-entropy loss between its predictions and the actual next characters in the training data. It uses the Adam optimizer and supports checkpointing to save and resume training.\n",
    "*   **Prediction (`run_pred`):** Given an input string, the model processes the last `seq_length` characters and outputs the top 3 most probable characters that could follow.\n",
    "\n",
    "**Use Case:**\n",
    "\n",
    "The primary use case is **next character prediction**. Given a segment of text, the model attempts to predict which character is most likely to appear next. This has applications in:\n",
    "*   **Text Autocompletion:** Suggesting subsequent characters or words.\n",
    "*   **Generative Text Models:** While this model predicts one character at a time, this is a fundamental building block for more complex text generation systems.\n",
    "*   **Understanding Language Structure:** Character-level modeling helps capture fundamental patterns and structures within different languages.\n",
    "\n",
    "The model is designed to be **multilingual**. By training it on a corpus containing texts from various languages (as demonstrated in the data download step of this notebook), it can learn to predict characters across those languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15aca9",
   "metadata": {},
   "source": [
    "# Running Multilingual Character-Level LSTM in Google Colab\n",
    "\n",
    "This notebook guides you through running the character-level LSTM model in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497becff",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability\n",
    "\n",
    "First, verify that Colab has assigned a GPU to your notebook. Go to **Runtime > Change runtime type** and select **GPU** as your hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ba4ea",
   "metadata": {},
   "source": [
    "### Mount Google Drive for Persistent Storage\n",
    "\n",
    "To ensure your project files, training data, and model checkpoints are saved persistently across sessions, we will work from your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the base path on your Google Drive where the project will reside\n",
    "# IMPORTANT: Create this directory in your Google Drive if it doesn't exist:\n",
    "# For example, My Drive > Colab Notebooks > cse517p_projects\n",
    "# Then set GDRIVE_BASE_PATH accordingly.\n",
    "import os\n",
    "GDRIVE_BASE_PATH = \"/content/drive/My Drive/Colab_Projects_CSE517P\" # CHANGE THIS TO YOUR PREFERRED GDRIVE PATH\n",
    "PROJECT_DIR_NAME = \"cse517p-project\"\n",
    "GDRIVE_PROJECT_PATH = os.path.join(GDRIVE_BASE_PATH, PROJECT_DIR_NAME)\n",
    "\n",
    "# Create the base directory on Drive if it doesn't exist\n",
    "if not os.path.exists(GDRIVE_BASE_PATH):\n",
    "    os.makedirs(GDRIVE_BASE_PATH)\n",
    "    print(f\"Created base directory: {GDRIVE_BASE_PATH}\")\n",
    "\n",
    "print(f\"Project will be set up in: {GDRIVE_PROJECT_PATH}\")\n",
    "# Note: The actual project directory (cse517p-project) will be created by cloning or manually in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8bbd2",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Project Repository on Google Drive\n",
    "\n",
    "To ensure persistence, your project files (including code, data, and saved models/checkpoints) must reside on your Google Drive.\n",
    "\n",
    "**Choose one of the options below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Clone your GitHub repository into Google Drive\n",
    "\n",
    "# Ensure the GDRIVE_PROJECT_PATH is defined from the cell above.\n",
    "# If the project directory already exists from a previous run, this cell might show an error or skip cloning.\n",
    "# You might want to remove the existing directory if you want a fresh clone:\n",
    "# !rm -rf \"$GDRIVE_PROJECT_PATH\"\n",
    "\n",
    "if not os.path.exists(GDRIVE_PROJECT_PATH):\n",
    "    print(f\"Cloning repository into {GDRIVE_PROJECT_PATH}...\")\n",
    "    # Clone into the parent of where GDRIVE_PROJECT_PATH should be, then cd into it\n",
    "    # Or, more simply, clone directly if GDRIVE_PROJECT_PATH is meant to be the repo root\n",
    "    %cd $GDRIVE_BASE_PATH \n",
    "    !git clone https://github.com/jamevaalet/cse517p-project.git $PROJECT_DIR_NAME # Replace with your repo URL\n",
    "    %cd $PROJECT_DIR_NAME\n",
    "    print(f\"Successfully cloned and changed directory to: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Project directory {GDRIVE_PROJECT_PATH} already exists. Skipping clone.\")\n",
    "    %cd $GDRIVE_PROJECT_PATH\n",
    "    print(f\"Changed directory to: {os.getcwd()}\")\n",
    "\n",
    "# Verify current directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3514351",
   "metadata": {},
   "source": [
    "Option 2: Upload files manually to Google Drive\n",
    "\n",
    "If you haven't pushed to GitHub, create the project structure on your Google Drive and upload files.\n",
    "Run the cell below to create the directory structure. Then, use the Colab file browser (left sidebar) to navigate to `/content/drive/My Drive/Your_Path/cse517p-project/` and upload your files into the `src`, `data`, etc., subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Create project directory structure on Google Drive\n",
    "\n",
    "# Ensure the GDRIVE_PROJECT_PATH is defined from a cell above.\n",
    "print(f\"Creating project structure in: {GDRIVE_PROJECT_PATH}\")\n",
    "\n",
    "# Create the main project directory if it doesn't exist\n",
    "if not os.path.exists(GDRIVE_PROJECT_PATH):\n",
    "    os.makedirs(GDRIVE_PROJECT_PATH)\n",
    "    print(f\"Created project directory: {GDRIVE_PROJECT_PATH}\")\n",
    "\n",
    "# Create subdirectories\n",
    "!mkdir -p \"$GDRIVE_PROJECT_PATH/src\"\n",
    "!mkdir -p \"$GDRIVE_PROJECT_PATH/data\"\n",
    "!mkdir -p \"$GDRIVE_PROJECT_PATH/work\"  # work directory for checkpoints and models\n",
    "!mkdir -p \"$GDRIVE_PROJECT_PATH/example\"\n",
    "!mkdir -p \"$GDRIVE_PROJECT_PATH/output\" # For test outputs\n",
    "\n",
    "%cd $GDRIVE_PROJECT_PATH\n",
    "print(f\"Changed directory to: {os.getcwd()}\")\n",
    "print(\"Please upload your files to the respective subdirectories (e.g., src, data) using the Colab file browser.\")\n",
    "\n",
    "# Verify current directory and structure\n",
    "!pwd\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b2e95",
   "metadata": {},
   "source": [
    "## Step 3: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24617e77",
   "metadata": {},
   "source": [
    "## Step 4: Download Multilingual Training Data\n",
    "\n",
    "Let's download sample multilingual data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data\n",
    "# This will download data into the 'data' subdirectory of your project on Google Drive\n",
    "# (assuming you have successfully cd'd into GDRIVE_PROJECT_PATH)\n",
    "!mkdir -p data\n",
    "\n",
    "# English sample (Pride and Prejudice)\n",
    "!curl -s https://www.gutenberg.org/files/1342/1342-0.txt > data/english_pride_prejudice.txt\n",
    "# Additional English samples\n",
    "!curl -s https://www.gutenberg.org/files/1661/1661-0.txt > data/english_sherlock_holmes.txt\n",
    "!curl -s https://www.gutenberg.org/files/2701/2701-0.txt > data/english_moby_dick.txt\n",
    "\n",
    "# Spanish sample (Don Quixote)\n",
    "!curl -s https://www.gutenberg.org/files/2000/2000-0.txt > data/spanish_don_quijote.txt\n",
    "# Additional Spanish samples\n",
    "!curl -s https://www.gutenberg.org/files/5946/5946-0.txt > data/spanish_la_regenta.txt # La Regenta by Clarín\n",
    "!curl -s https://www.gutenberg.org/files/1701/1701-0.txt > data/spanish_fortunata_y_jacinta.txt # Fortunata y Jacinta by Benito Pérez Galdós\n",
    "\n",
    "\n",
    "# French sample (Les Misérables - Tome I)\n",
    "!curl -s https://www.gutenberg.org/files/17489/17489-0.txt > data/french_les_miserables.txt\n",
    "# Additional French samples\n",
    "!curl -s https://www.gutenberg.org/files/2413/2413-0.txt > data/french_madame_bovary.txt # Madame Bovary by Flaubert\n",
    "!curl -s https://www.gutenberg.org/files/10003/10003-0.txt > data/french_le_rouge_et_le_noir.txt # Le Rouge et le Noir by Stendhal\n",
    "\n",
    "# German sample (Also sprach Zarathustra)\n",
    "!curl -s https://www.gutenberg.org/cache/epub/1998/pg1998.txt > data/german_zarathustra.txt\n",
    "# Additional German samples\n",
    "!curl -s https://www.gutenberg.org/files/5200/5200-0.txt > data/german_die_verwandlung.txt # Die Verwandlung by Kafka\n",
    "!curl -s https://www.gutenberg.org/files/2229/2229-0.txt > data/german_faust_part1.txt # Faust I by Goethe\n",
    "\n",
    "# Portuguese sample (Os Lusíadas)\n",
    "!curl -s https://www.gutenberg.org/files/3333/3333-0.txt > data/portuguese_lusiadas.txt\n",
    "# Additional Portuguese samples\n",
    "!curl -s https://www.gutenberg.org/files/5518/5518-0.txt > data/portuguese_dom_casmurro.txt # Dom Casmurro by Machado de Assis\n",
    "!curl -s https://www.gutenberg.org/files/54706/54706-0.txt > data/portuguese_memorias_postumas_bras_cubas.txt # Memórias Póstumas de Brás Cubas by Machado de Assis\n",
    "\n",
    "\n",
    "# Commenting out other Latin-script languages to focus on the top 5 spoken ones:\n",
    "# Italian sample (La Divina Commedia di Dante)\n",
    "# !curl -s https://www.gutenberg.org/files/1001/1001-0.txt > data/italian_divina_commedia.txt\n",
    "# Dutch sample (Max Havelaar)\n",
    "# !curl -s https://www.gutenberg.org/files/36000/36000-0.txt > data/dutch_max_havelaar.txt\n",
    "# Swedish sample (Röda rummet)\n",
    "# !curl -s https://www.gutenberg.org/files/5381/5381-0.txt > data/swedish_roda_rummet.txt\n",
    "# Finnish sample (Seitsemän veljestä)\n",
    "# !curl -s https://www.gutenberg.org/files/11961/11961-0.txt > data/finnish_seitseman_veljesta.txt\n",
    "# Danish sample (Niels Lyhne)\n",
    "# !curl -s https://www.gutenberg.org/files/19099/19099-0.txt > data/danish_niels_lyhne.txt\n",
    "# Norwegian sample (Peer Gynt)\n",
    "# !curl -s https://www.gutenberg.org/files/2339/2339-0.txt > data/norwegian_peer_gynt.txt\n",
    "# Polish sample (Pan Tadeusz)\n",
    "# !curl -s https://www.gutenberg.org/files/20933/20933-0.txt > data/polish_pan_tadeusz.txt\n",
    "# Hungarian sample (Az arany ember)\n",
    "# !curl -s https://www.gutenberg.org/files/20925/20925-0.txt > data/hungarian_az_arany_ember.txt\n",
    "# Latin sample (Commentarii de Bello Gallico)\n",
    "# !curl -s https://www.gutenberg.org/files/10657/10657-0.txt > data/latin_bello_gallico.txt\n",
    "\n",
    "# Non-Latin script languages removed from download:\n",
    "# Russian, Chinese, Japanese, Korean, Arabic, Hindi\n",
    "\n",
    "# Create example input file for testing\n",
    "# This will create files in the 'example' subdirectory of your project on Google Drive\n",
    "!mkdir -p example\n",
    "!echo \"Hello, how are you\" > example/input.txt\n",
    "!echo \"Bonjour mon ami\" >> example/input.txt\n",
    "!echo \"Hola, ¿cómo estás\" >> example/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac351da",
   "metadata": {},
   "source": [
    "### Languages in the Training Data\n",
    "\n",
    "The preceding cell downloads sample texts from Project Gutenberg to serve as training data. Based on these downloads, the model will be trained on content from the following **top 5 most spoken Latin-script languages**:\n",
    "\n",
    "*   English\n",
    "*   Spanish\n",
    "*   French\n",
    "*   German\n",
    "*   Portuguese\n",
    "\n",
    "The `MyModel.load_training_data()` method in `src/myprogram.py` will load all `.txt` files from the `data/` directory. The character vocabulary and subsequent training will be based on the combined content of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68de09",
   "metadata": {},
   "source": [
    "#### Script Usage in Downloaded Languages\n",
    "\n",
    "All languages listed above and downloaded for training primarily use **Latin-based scripts**. This focused approach aims to build a model specialized in these specific widely-spoken scripts.\n",
    "\n",
    "By focusing on these 5 Latin-script languages, the vocabulary will be smaller and potentially allow the model to better learn the nuances within these scripts given a fixed model capacity and dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924f50e",
   "metadata": {},
   "source": [
    "#### Top 5 Spoken Latin-Script Languages in the Training Set\n",
    "\n",
    "The training set for this notebook now exclusively comprises these top 5 most spoken Latin-script languages (by total number of speakers worldwide):\n",
    "\n",
    "1.  **English**\n",
    "2.  **Spanish**\n",
    "3.  **French**\n",
    "4.  **Portuguese**\n",
    "5.  **German**\n",
    "\n",
    "The model will be exposed to texts only from these languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c35142",
   "metadata": {},
   "source": [
    "## Step 5: If needed, create or upload the Python files\n",
    "\n",
    "If you cloned from GitHub, skip this step. Otherwise, you need to upload or create your Python files in the `src` directory.\n",
    "\n",
    "Click on the folder icon on the left sidebar, navigate to the `src` directory, and upload your `myprogram.py` and `predict.sh` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddc780",
   "metadata": {},
   "source": [
    "### Note on Training Timeouts and Resuming\n",
    "\n",
    "To mitigate Colab timeouts, the training script now saves a checkpoint (`work/checkpoint.pt`) after each epoch. This checkpoint includes:\n",
    "*   The model's learned weights.\n",
    "*   The state of the optimizer.\n",
    "*   The vocabulary (character-to-index mapping) used during training.\n",
    "*   The number of the last completed epoch.\n",
    "\n",
    "**How Resuming Works:**\n",
    "*   If your session disconnects, **ensure your Google Drive is remounted** if necessary (run the Drive mount cell again). Then, navigate back to your project directory on Drive (`%cd /content/drive/My Drive/Your_Path/cse517p-project`).\n",
    "*   Simply re-run the training cell (`!python src/myprogram.py train --work_dir work`).\n",
    "*   The script will automatically detect `work/checkpoint.pt` (which is now on your Google Drive). If found, it loads the saved progress and resumes training from the next epoch.\n",
    "*   **Data Handling on Resume:**\n",
    "    *   The **vocabulary** from the checkpoint is reloaded. This ensures character encodings remain consistent.\n",
    "    *   The raw **training data files are re-read** from the `data/` directory (on your Google Drive) at the start of the resumed session. If you've modified the contents of the `data/` directory (e.g., added more text files), the resumed training will use this updated set of files. However, any new characters in these files not present in the loaded vocabulary will be filtered out.\n",
    "    *   The `DataLoader` shuffles the dataset at the beginning of each epoch, including resumed epochs.\n",
    "*   **Final Model:** Once all epochs are completed, the final trained model will be saved as `work/model.pt` and `work/vocab.pt` on your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4455c",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "!python src/myprogram.py train --work_dir work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4e86f",
   "metadata": {},
   "source": [
    "### Inspect Model Vocabulary (After Training)\n",
    "\n",
    "After training, the model saves its vocabulary. Let's load it to see what characters it learned.\n",
    "If you haven't trained the model yet in this session, this cell might show information from a previously saved `vocab.pt` or fail if no such file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Ensure you are in the project directory\n",
    "if os.path.basename(os.getcwd()) != PROJECT_DIR_NAME and os.path.exists(GDRIVE_PROJECT_PATH):\n",
    "    %cd $GDRIVE_PROJECT_PATH\n",
    "\n",
    "vocab_path = os.path.join('work', 'vocab.pt')\n",
    "if os.path.exists(vocab_path):\n",
    "    vocab_info = torch.load(vocab_path, map_location='cpu') # Load to CPU for inspection\n",
    "    char_to_idx = vocab_info['char_to_idx']\n",
    "    idx_to_char = vocab_info['idx_to_char']\n",
    "    vocab_size = vocab_info['vocab_size']\n",
    "    \n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    print(\"\\nSample of idx_to_char mapping (first 100 entries):\")\n",
    "    for i in range(min(100, vocab_size)):\n",
    "        print(f\"{i}: '{idx_to_char[i]}'\", end='  ')\n",
    "        if (i+1) % 10 == 0:\n",
    "            print() # Newline every 10 chars\n",
    "    print(\"\\n\\nChecking for specific characters (example):\")\n",
    "    example_chars = ['a', 'z', 'A', 'Z', ' ', '.', 'ñ', 'ç', 'ü', 'Привет', '你好', 'こんにちは', '안녕하세요', 'مرحبا', 'नमस्ते']\n",
    "    for char_set in example_chars:\n",
    "        for char_to_check in char_set: # Iterate if it's a string like \"Привет\"\n",
    "             present = \"Present\" if char_to_check in char_to_idx else \"Absent\"\n",
    "             idx = char_to_idx.get(char_to_check, \"N/A\")\n",
    "             print(f\"Character '{char_to_check}': {present} (Index: {idx})\")\n",
    "else:\n",
    "    print(f\"Vocabulary file not found at {vocab_path}. Train the model first or ensure the path is correct.\")\n",
    "\n",
    "# Alternative: If you want to inspect vocab from raw data without relying on a trained model's vocab.pt\n",
    "# This requires access to the CharDataset class and training data.\n",
    "# from src.myprogram import CharDataset, MyModel\n",
    "# print(\"\\nInspecting vocabulary from raw training data:\")\n",
    "# raw_training_data = MyModel.load_training_data() # Loads and cleans\n",
    "# if raw_training_data:\n",
    "#     temp_dataset = CharDataset(raw_training_data)\n",
    "#     print(f\"Raw Data Vocab Size: {temp_dataset.vocab_size}\")\n",
    "#     print(\"\\nSample of raw data idx_to_char (first 100):\")\n",
    "#     for i in range(min(100, temp_dataset.vocab_size)):\n",
    "#         print(f\"{i}: '{temp_dataset.idx_to_char[i]}'\", end='  ')\n",
    "#         if (i+1) % 10 == 0:\n",
    "#             print()\n",
    "#     print()\n",
    "# else:\n",
    "#     print(\"Could not load raw training data for vocab inspection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bba38",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de478e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "!mkdir -p output\n",
    "\n",
    "# Run prediction\n",
    "!python src/myprogram.py test --work_dir work --test_data example/input.txt --test_output output/pred.txt\n",
    "\n",
    "# Display predictions\n",
    "print(\"Input text:\")\n",
    "!cat example/input.txt\n",
    "\n",
    "print(\"\\nPredictions (top 3 next characters):\")\n",
    "!cat output/pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2269c",
   "metadata": {},
   "source": [
    "## Step 7.1: Comprehensive Multilingual Test (Optional)\n",
    "\n",
    "This step uses a more diverse set of input strings covering the languages downloaded in Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1255d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multilingual input and answer files on Google Drive (within the Colab environment)\n",
    "\n",
    "input_multi_content = \"\"\"Hello, how are yo\n",
    "Hola, ¿cómo está\n",
    "Bonjour mon am\n",
    "Guten Tag, wie geh\n",
    "Olá, como est\"\"\"\n",
    "\n",
    "answer_multi_content = \"\"\"u\n",
    "s\n",
    "i\n",
    "t\n",
    "á\"\"\"\n",
    "\n",
    "with open(\"example/input_multi.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(input_multi_content)\n",
    "\n",
    "with open(\"example/answer_multi.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(answer_multi_content)\n",
    "\n",
    "print(\"Created example/input_multi.txt and example/answer_multi.txt (Top 5 Latin-script focused)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab00ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction with the new multilingual test file\n",
    "!python src/myprogram.py test --work_dir work --test_data example/input_multi.txt --test_output output/pred_multi.txt\n",
    "\n",
    "# Display results\n",
    "print(\"Multilingual Input text (example/input_multi.txt):\")\n",
    "!cat example/input_multi.txt\n",
    "\n",
    "print(\"\\nMultilingual Predictions (output/pred_multi.txt - top 3 next characters):\")\n",
    "!cat output/pred_multi.txt\n",
    "\n",
    "print(\"\\nMultilingual Gold Answers (example/answer_multi.txt):\")\n",
    "!cat example/answer_multi.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071628b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade the multilingual predictions\n",
    "# Ensure the grader script is available. If you cloned the repo, it should be in `grader/grade.py`.\n",
    "# If not, you might need to upload it or adjust the path.\n",
    "# Assuming GDRIVE_PROJECT_PATH is your project root where 'grader' directory exists.\n",
    "\n",
    "GRADER_SCRIPT_PATH = \"grader/grade.py\" # Path relative to GDRIVE_PROJECT_PATH\n",
    "\n",
    "if os.path.exists(GRADER_SCRIPT_PATH):\n",
    "    print(\"\\nGrading multilingual predictions:\")\n",
    "    !python $GRADER_SCRIPT_PATH output/pred_multi.txt example/answer_multi.txt --verbose\n",
    "else:\n",
    "    print(f\"\\nGrader script not found at {GRADER_SCRIPT_PATH}. Skipping multilingual grading.\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(\"Ensure 'grader/grade.py' exists in your project directory on Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13f5d1",
   "metadata": {},
   "source": [
    "## Step 8: Save the Trained Model\n",
    "\n",
    "If you want to save your trained model from Colab to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6cc5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the work directory which contains your model (now on Google Drive)\n",
    "# The zip file will be created in the current directory (your project root on Drive)\n",
    "!zip -r trained_model.zip work/\n",
    "\n",
    "# Download the model (click the link that appears)\n",
    "# This downloads the zip file from your Colab environment's view of Google Drive\n",
    "from google.colab import files\n",
    "files.download('trained_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8531649",
   "metadata": {},
   "source": [
    "## Optional: Experiment with Hyperparameters\n",
    "\n",
    "You can modify the training hyperparameters by editing the code or passing additional arguments. Here's an example of how to modify key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8affed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary modified version of the program with different hyperparameters\n",
    "%%writefile src/modified_program.py\n",
    "# Import the original program\n",
    "from src.myprogram import *\n",
    "\n",
    "# Override the run_train method to use different hyperparameters\n",
    "def custom_run_train(self, data, work_dir):\n",
    "    # Create dataset with smaller sequence length\n",
    "    seq_length = 32  # Smaller sequence length\n",
    "    self.dataset = CharDataset(data, seq_length)\n",
    "    \n",
    "    # Create model with different parameters\n",
    "    vocab_size = self.dataset.vocab_size\n",
    "    embedding_dim = 64     # Smaller embedding size\n",
    "    hidden_dim = 128       # Smaller hidden dimension\n",
    "    num_layers = 1         # Fewer layers\n",
    "    self.model = CharLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "    self.model.to(self.device)\n",
    "    \n",
    "    # Modified training parameters\n",
    "    batch_size = 32        # Smaller batch size\n",
    "    num_epochs = 5         # Fewer epochs\n",
    "    learning_rate = 0.005  # Higher learning rate\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (sequences, labels) in enumerate(dataloader):\n",
    "            sequences, labels = sequences.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Apply the monkey patch\n",
    "MyModel.run_train = custom_run_train\n",
    "\n",
    "# Run with the modified script\n",
    "if __name__ == '__main__':\n",
    "    # Use the same main code from the original program\n",
    "    from src.myprogram import *\n",
    "    if __name__ == '__main__' and globals()['__name__'] == '__main__':\n",
    "        parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "        parser.add_argument('mode', choices=('train', 'test'), help='what to run')\n",
    "        parser.add_argument('--work_dir', help='where to save', default='work_modified')\n",
    "        parser.add_argument('--test_data', help='path to test data', default='example/input.txt')\n",
    "        parser.add_argument('--test_output', help='path to write test predictions', default='pred_modified.txt')\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        random.seed(0)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            if not os.path.isdir(args.work_dir):\n",
    "                print('Making working directory {}'.format(args.work_dir))\n",
    "                os.makedirs(args.work_dir)\n",
    "            print('Instatiating model')\n",
    "            model = MyModel()\n",
    "            print('Loading training data')\n",
    "            train_data = MyModel.load_training_data()\n",
    "            print('Training')\n",
    "            model.run_train(train_data, args.work_dir)\n",
    "            print('Saving model')\n",
    "            model.save(args.work_dir)\n",
    "        elif args.mode == 'test':\n",
    "            print('Loading model')\n",
    "            model = MyModel.load(args.work_dir)\n",
    "            print('Loading test data from {}'.format(args.test_data))\n",
    "            test_data = MyModel.load_test_data(args.test_data)\n",
    "            print('Making predictions')\n",
    "            pred = model.run_pred(test_data)\n",
    "            print('Writing predictions to {}'.format(args.test_output))\n",
    "            assert len(pred) == len(test_data), 'Expected {} predictions but got {}'.format(len(test_data), len(pred))\n",
    "            model.write_pred(pred, args.test_output)\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown mode {}'.format(args.mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56103999",
   "metadata": {},
   "source": [
    "Run the modified version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507afc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with modified hyperparameters\n",
    "!python src/modified_program.py train --work_dir work_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b01e81",
   "metadata": {},
   "source": [
    "## Running with Docker in Google Colab\n",
    "\n",
    "You can also run your model using Docker inside Google Colab, which ensures the exact same environment as your local setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040bb818",
   "metadata": {},
   "source": [
    "### 1. Install Docker in Colab\n",
    "\n",
    "First, we need to install Docker in the Colab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5707fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any old Docker installations\n",
    "!apt-get remove docker docker-engine docker.io containerd runc\n",
    "\n",
    "# Install prerequisites\n",
    "!apt-get update\n",
    "!apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release\n",
    "\n",
    "# Add Docker's official GPG key\n",
    "!curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "\n",
    "# Set up the stable repository\n",
    "!echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "\n",
    "# Install Docker Engine\n",
    "!apt-get update\n",
    "!apt-get install -y docker-ce docker-ce-cli containerd.io\n",
    "\n",
    "# Verify Docker installation\n",
    "!docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fcb31",
   "metadata": {},
   "source": [
    "### 2. Start Docker service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Docker service\n",
    "!service docker start\n",
    "\n",
    "# Check Docker status\n",
    "!service docker status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58484bfd",
   "metadata": {},
   "source": [
    "### 3. Create Project Files for Docker\n",
    "\n",
    "We need to create all necessary files for our Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857be0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile\n",
    "%%writefile Dockerfile\n",
    "# Using the latest PyTorch image with CUDA support\n",
    "FROM pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime\n",
    "\n",
    "RUN mkdir /job\n",
    "WORKDIR /job\n",
    "VOLUME [\"/job/data\", \"/job/src\", \"/job/work\", \"/job/output\"]\n",
    "\n",
    "# Install dependencies using requirements.txt\n",
    "COPY requirements.txt /job/\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc311a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt\n",
    "%%writefile requirements.txt\n",
    "numpy>=1.20.0\n",
    "tqdm>=4.64.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predict.sh script\n",
    "%%writefile src/predict.sh\n",
    "#!/usr/bin/env bash\n",
    "set -e\n",
    "set -v\n",
    "python src/myprogram.py test --work_dir work --test_data $1 --test_output $2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a78f09",
   "metadata": {},
   "source": [
    "### 4. Build Docker Image\n",
    "\n",
    "Now we can build the Docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27269661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Docker image\n",
    "!docker build -t cse517-proj/mylstm -f Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c5e1c",
   "metadata": {},
   "source": [
    "### 5. Run Training with Docker\n",
    "\n",
    "Now we can train our model using Docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories exist (they should, if set up on Drive)\n",
    "# These commands will operate relative to your project directory on Drive\n",
    "!mkdir -p data work output example\n",
    "\n",
    "# Check if chmod is needed for the script\n",
    "!chmod +x src/predict.sh\n",
    "\n",
    "# Run training with Docker\n",
    "# Note: Docker volume mounts will now point to paths on your Google Drive via Colab's mount\n",
    "!docker run --rm \\\n",
    "  -v \"$PWD/src\":/job/src \\\n",
    "  -v \"$PWD/data\":/job/data \\\n",
    "  -v \"$PWD/work\":/job/work \\\n",
    "  cse517-proj/mylstm bash -c \"cd /job && python src/myprogram.py train --work_dir work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa51a9c",
   "metadata": {},
   "source": [
    "### 6. Run Testing with Docker\n",
    "\n",
    "Now we can test our model using Docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data if it doesn't exist (in 'example' on Drive)\n",
    "!mkdir -p example\n",
    "!echo \"Hello, how are you\" > example/input.txt\n",
    "!echo \"Bonjour mon ami\" >> example/input.txt\n",
    "!echo \"Hola, ¿cómo estás\" >> example/input.txt\n",
    "\n",
    "# Run testing with Docker\n",
    "# Volume mounts point to paths on your Google Drive\n",
    "!docker run --rm \\\n",
    "  -v \"$PWD/src\":/job/src \\\n",
    "  -v \"$PWD/work\":/job/work \\\n",
    "  -v \"$PWD/example\":/job/data \\\n",
    "  -v \"$PWD/output\":/job/output \\\n",
    "  cse517-proj/mylstm bash /job/src/predict.sh /job/data/input.txt /job/output/pred.txt\n",
    "\n",
    "# Display results (from files on Drive)\n",
    "print(\"Input:\")\n",
    "!cat example/input.txt\n",
    "print(\"\\nPredictions:\")\n",
    "!cat output/pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9cd60",
   "metadata": {},
   "source": [
    "### 7. Compare Docker vs. Direct Execution\n",
    "\n",
    "You can now compare the results between running directly in Colab versus running in Docker. Both should produce similar results, but Docker ensures better reproducibility and consistency with your local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881999f",
   "metadata": {},
   "source": [
    "### Notes on Running Docker in Colab\n",
    "\n",
    "1. Docker in Colab requires administrative privileges, which Google provides.\n",
    "2. The Docker installation process might take a few minutes.\n",
    "3. If you encounter memory issues, try reducing batch sizes or sequence lengths.\n",
    "4. Docker containers are ephemeral - data will be lost when the container stops unless mounted as volumes.\n",
    "5. Colab sessions have time limits - save your model frequently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
